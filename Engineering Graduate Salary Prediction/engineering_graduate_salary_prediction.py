# -*- coding: utf-8 -*-
"""Engineering Graduate Salary Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GD5CgZzxlCp43hu5upJR6MOzikWrA5kG
"""

from google.colab import drive
drive.mount('/content/drive')

"""#**Importing libraries**"""

!pip install autoviz          # for automatic visualization of any dataset with just one line of code. 
!pip install -U --pre pycaret # for regression models comparison

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from statsmodels.stats.outliers_influence import variance_inflation_factor
from autoviz.classify_method import data_cleaning_suggestions ,data_suggestions
from pycaret  import regression
from sklearn.model_selection import cross_val_score

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Engineering_graduate_salary.csv', delimiter=",")

data.head()

data.info()

data.columns

data.describe()

"""#**Data cleaning**

"""

data_cleaning_suggestions(data)

# These columns are not way useful in prediction, because they won't affect the salary
data.drop(['ID','DOB','CollegeID','12graduation','10board','12board','CollegeState','CollegeCityID','CollegeCityTier','GraduationYear'], axis=1, inplace=True)

duplicate = data[data.duplicated()]
duplicate                           # No duplicates

df = data.drop_duplicates()
df.shape 
data = data.drop_duplicates()

data.dropna(inplace=True)

data.Degree.value_counts()  # checking counts of each degree

specialization = data.Specialization.value_counts()
specialization

# seeing specialization there are many specializations which have 1 row, 
# so combining these many individual specializations into one increases performance
specializationlessthan10 = specialization[specialization<=10]
specializationlessthan10

def removespecialessthan10(value):
    if value in specializationlessthan10:
        return 'OTHERS'
    else:
        return value

data.Specialization = data.Specialization.apply(removespecialessthan10)
data.Specialization.value_counts()

# working with GPA

data.collegeGPA.value_counts()

"""#**Data Visualization**"""

# As there are many values in GPA column, so we try to plot graphs so that we can see outliers and visualise more
data

plt.scatter(df.index, df.collegeGPA)

# As there are some outliers in the collegeGPA column, removing all rows where collegeGPA 
# is less than or equal to 40. This will help the model to perform better

data = data[(data['collegeGPA'] > 40)]  # so considering columns only > 40
data.shape

# visualising domain, computer programming, Electronics and Semicon cols
plt.figure(figsize = (12,12))

plt.subplot(2,3,1)
plt.scatter(data.index, data.Domain)
plt.title('Domain')

plt.subplot(2,3,2)
plt.scatter(data.index, data.ComputerProgramming)
plt.title('Computer Programming')

plt.subplot(2,3,3)
plt.scatter(data.index, data.ElectronicsAndSemicon)
plt.title('Electronics and Semicon')

plt.show()

data.describe()

# from the above scatter plots we can see some values are starting with -1 replacing -1 with null

data = data.replace(-1, np.nan)

data.isnull().any()

data.isnull().values.sum() > 0

# Filling null values with mean
# We could have directly filled -1 with mean but, we need to fill mean values pertaining to that column
# Hence, we now fill with NAN and then replacing them with mean column value

cols_with_nan = [col for col in df.columns if data.isna().sum()[col] > 0]

for column in cols_with_nan:
    data[column] = data[column].fillna(data[column].mean())

data.isnull().any()

# visualising domain, computer programming, Electronics and Semicon cols
plt.figure(figsize = (12,12))

plt.subplot(2,3,1)
plt.scatter(data.index, data.Domain)
plt.title('Domain')

plt.subplot(2,3,2)
plt.scatter(data.index, data.ComputerProgramming)
plt.title('Computer Programming')

plt.subplot(2,3,3)
plt.scatter(data.index, data.ElectronicsAndSemicon)
plt.title('Electronics and Semicon')

plt.show()

# Gender visualisation

data.Gender.value_counts()
sns.countplot(data=data, x=data.Gender)

plt.figure(figsize=(15,6))
sns.countplot(data=data, x= data['Specialization'])
plt.xticks(rotation = 90)

# plotting GPA vs Salary considering specialization

plt.figure(figsize=(15,8))
sns.scatterplot(data = data, x=data.collegeGPA, y=data.Salary, hue = data.Specialization)

# we can see mostly CSE, ICE, CE, ECE has more frequency

# plotting GPA vs Salary considering degree 

plt.figure(figsize=(15,6))
sns.scatterplot(data = data, x=data.collegeGPA, y=data.Salary, hue = data.Degree, palette ='magma')

# Opeenness to experience vs salary

plt.figure(figsize=(15,6))
sns.scatterplot(data = data, x=data.openess_to_experience, y=data.Salary)

# this heatmap tells which values are mostly used(dark colored)
plt.figure(figsize=(17,10))
sns.heatmap(data.corr(), annot=True)

# pair plot gives plot between every column with every other column
sns.pairplot(data)

"""#**Data Preprocessing**"""

for col in data.columns:
  if data[col].dtype == 'object':
    print(col)

# label en coding categorical columns
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
data.Gender = le.fit_transform(data.Gender)
data.Degree = le.fit_transform(data.Degree)
data.Specialization = le.fit_transform(data.Specialization)

df.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = data.drop('Salary', axis = 1)
y = data['Salary']

# The setup function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. 
# setup must be called before executing any other function in pycaret. 
# It takes two mandatory parameters: a pandas dataframe and the name of the target column.
regression.setup(X,target=y)

"""##**source:** https://towardsdatascience.com/introduction-to-regression-in-python-with-pycaret-d6150b540fc4

**session_id**: A pseudo-random number distributed as a seed in all functions for later reproducibility. If no session_id is passed, a random number is automatically generated that is distributed to all functions. In this experiment, the session_id is set as 123 for later reproducibility.

**Original Data**: Displays the original shape of the dataset. For this experiment, (5400, 8) means 5400 samples and 8 features including the target column.

**Missing Values**: When there are missing values in the original data, this will show as True. For this experiment, there are no missing values in the dataset.

**Numeric Features**: Number of features inferred as numeric. In this dataset, 1 out of 8 features is inferred as numeric.

**Categorical Features**: Number of features inferred as categorical. In this dataset, 6 out of 8 features are inferred as categorical.

**Transformed Train Set**: Displays the shape of the transformed training set. 

**Transformed Test Set**: Displays the shape of the transformed test/hold-out set.
"""

regression.compare_models()

"""**MAE:** Mean Absolute Error - should be less

**MSE:** Mean Squared Error - should be less

**RMSE:** Root Mean Squared Error - should be less

**RMSLE:** Root Mean Squared Logarithmic Error (Root Mean Squared Logarithmic Error is calculated by applying log to the actual and the predicted values and then taking their differences, https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)

**MAPE:** Mean Absolute Percentage - should be less

**R2 score:** - should be between 0.10 and 0.50
"""

sc = StandardScaler()
X = sc.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True,random_state=1974)

from sklearn.linear_model import HuberRegressor, Ridge
h = HuberRegressor()
r = Ridge(alpha = 0.01)

# comparing Ridge and Huber regression according to table

r.fit(X_train, y_train)
h.fit(X_train, y_train)

p1 = r.predict(X_test)
p2 = h.predict(X_test)

r_score = r.score(X_test, y_test)
h_score = h.score(X_test, y_test)

r_score

h_score

data.drop('Salary', axis = 1).columns

np.array(data.iloc[1])

temp = np.asarray(( [ 1.00000000e+00,  5.70000000e+01,  6.45000000e+01,  2.00000000e+00,
        0.00000000e+00,  5.00000000e+00,  6.50000000e+01,  4.40000000e+02,
        4.35000000e+02,  2.10000000e+02,  3.42314900e-01,  3.65000000e+02,
        3.35947917e+02,  4.06720630e+02,  4.01174863e+02,  4.23336066e+02,
        3.49879562e+02,  3.41960000e+02,  1.13360000e+00,  4.59000000e-02,
        1.23960000e+00,  5.26200000e-01, -2.85900000e-01]))
temp = temp.reshape(1, -1)
std_data = sc.transform(temp)
std_data

r.predict(std_data)